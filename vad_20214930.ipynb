{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 구글 드라이브의 파일 다운로드를 위한 패키지 설치\n!pip install gdown\n# 구글 드라이브로 공유된 zip 파일을 colab 환경에 직접 다운로드\n!gdown --id 1dCMUGl1sNj0hOZkuBXlk3ounl_QkD5Xz  -O data.zip\n# data.zip 파일을 data라는 폴더에 압축풀기\n!unzip -q data.zip -d data\n# data.zip 파일 및 data 폴더가 다운로드 및 생성되었는지 확인\n!ls -al","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:44:14.080168Z","iopub.execute_input":"2024-11-25T14:44:14.081022Z","iopub.status.idle":"2024-11-25T14:48:26.406496Z","shell.execute_reply.started":"2024-11-25T14:44:14.080983Z","shell.execute_reply":"2024-11-25T14:48:26.405378Z"}},"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1dCMUGl1sNj0hOZkuBXlk3ounl_QkD5Xz\nFrom (redirected): https://drive.google.com/uc?id=1dCMUGl1sNj0hOZkuBXlk3ounl_QkD5Xz&confirm=t&uuid=565bc6e6-ed70-4895-95b4-f058592555d4\nTo: /kaggle/working/data.zip\n100%|██████████████████████████████████████| 7.25G/7.25G [02:37<00:00, 45.9MB/s]\ntotal 7078444\ndrwxr-xr-x 4 root root       4096 Nov 25 14:47 .\ndrwxr-xr-x 5 root root       4096 Nov 25 14:42 ..\ndrwxr-xr-x 2 root root       4096 Nov 25 14:42 .virtual_documents\ndrwxr-xr-x 4 root root       4096 Nov 25 14:48 data\n-rw-r--r-- 1 root root 7248304333 Nov 25 10:11 data.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:52:03.965061Z","iopub.execute_input":"2024-11-25T14:52:03.966003Z","iopub.status.idle":"2024-11-25T14:52:05.804685Z","shell.execute_reply.started":"2024-11-25T14:52:03.965968Z","shell.execute_reply":"2024-11-25T14:52:05.803777Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"pip install librosa numpy pandas scikit-learn tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:52:08.370988Z","iopub.execute_input":"2024-11-25T14:52:08.371807Z","iopub.status.idle":"2024-11-25T14:52:17.397844Z","shell.execute_reply.started":"2024-11-25T14:52:08.371759Z","shell.execute_reply":"2024-11-25T14:52:17.396862Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.10.2.post1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\nRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.14.1)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.60.0)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.12.2)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.8)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lazy-loader>=0.1->librosa) (21.3)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.11.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->lazy-loader>=0.1->librosa) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# 경로 설정\nBASE_PATH = \"/kaggle/working/data\"\nTRAIN_PATH = os.path.join(BASE_PATH, \"train\")\nTEST_PATH = os.path.join(BASE_PATH, \"test\")\n\nclass VADDataset(Dataset):\n    def __init__(self, base_path, is_train=True):\n        if is_train:\n            self.wav_path = os.path.join(TRAIN_PATH, 'wav160')\n            self.text_path = os.path.join(TRAIN_PATH, 'text')\n            self.file_list = sorted(os.listdir(self.wav_path))\n        else:\n            self.wav_path = os.path.join(TEST_PATH, 'wav160')\n            self.file_list = sorted(os.listdir(self.wav_path))\n        \n        print(f\"{'Train' if is_train else 'Test'} WAV path: {self.wav_path}\")\n        if is_train:\n            print(f\"Train TEXT path: {self.text_path}\")\n        print(f\"Number of files: {len(self.file_list)}\")\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, idx):\n        wav_file = self.file_list[idx]\n        wav_path = os.path.join(self.wav_path, wav_file)\n        \n        # 오디오 로드\n        audio, sr = librosa.load(wav_path, sr=16000)\n        \n        # MFCC 특성 추출 (프레임 길이와 홉 길이 명시)\n        mfcc = librosa.feature.mfcc(\n            y=audio, \n            sr=sr, \n            n_mfcc=13,\n            n_fft=400,      # 25ms at 16kHz\n            hop_length=160,  # 10ms at 16kHz\n            n_mels=40\n        )\n        mfcc = mfcc.T  # (time, features)\n        \n        # 에너지 특성 추출 (동일한 프레임 길이와 홉 길이 사용)\n        frame_length = 400\n        hop_length = 160\n        \n        # RMS 에너지 계산\n        rms = librosa.feature.rms(\n            y=audio,\n            frame_length=frame_length,\n            hop_length=hop_length\n        )\n        rms = rms.T  # (time, 1)\n        \n        # Zero Crossing Rate 추가\n        zcr = librosa.feature.zero_crossing_rate(\n            y=audio,\n            frame_length=frame_length,\n            hop_length=hop_length\n        )\n        zcr = zcr.T  # (time, 1)\n        \n        # 모든 특성 결합\n        features = np.concatenate([mfcc, rms, zcr], axis=1)  # (time, features)\n        \n        # 패딩이나 자르기로 시퀀스 길이 고정\n        target_length = 400  # 4초 * 100 프레임/초\n        if features.shape[0] < target_length:\n            pad_width = ((0, target_length - features.shape[0]), (0, 0))\n            features = np.pad(features, pad_width, mode='constant')\n        else:\n            features = features[:target_length]\n            \n        features = torch.FloatTensor(features)\n        \n        if hasattr(self, 'text_path'):  # train 데이터\n            txt_file = wav_file.replace('.wav', '.txt')\n            txt_path = os.path.join(self.text_path, txt_file)\n            \n            with open(txt_path, 'r') as f:\n                content = f.read().strip()\n                onset, offset, sound_class = content.split('\\t')\n                \n                if 'speech' in sound_class:\n                    center = (float(onset) + float(offset)) / 2\n                else:\n                    center = -1.0\n                    \n                return features, torch.FloatTensor([center])\n        else:\n            return features\n\nclass VADModel(nn.Module):\n    def __init__(self, input_size):\n        super(VADModel, self).__init__()\n        \n        self.lstm = nn.LSTM(\n            input_size=input_size, \n            hidden_size=64,\n            num_layers=2,\n            batch_first=True,\n            bidirectional=True,\n            dropout=0.3\n        )\n        \n        self.attention = nn.Sequential(\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(32, 1)\n        )\n    \n    def forward(self, x):\n        # LSTM 처리\n        lstm_out, _ = self.lstm(x)  # (batch, time, 128)\n        \n        # Attention 가중치 계산\n        attention_weights = self.attention(lstm_out)  # (batch, time, 1)\n        \n        # Attention 적용\n        weighted = lstm_out * attention_weights\n        context = weighted.sum(dim=1)  # (batch, 128)\n        \n        # 최종 예측\n        output = self.fc(context)\n        return output\n\ndef train_model(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    \n    for features, targets in tqdm(train_loader):\n        features, targets = features.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, targets)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(train_loader)\n\ndef predict(model, test_loader, device):\n    model.eval()\n    predictions = []\n    \n    with torch.no_grad():\n        for features in tqdm(test_loader):\n            if isinstance(features, tuple):\n                features = features[0]\n            features = features.to(device)\n            outputs = model(features)\n            predictions.extend(outputs.cpu().numpy())\n    \n    return np.array(predictions)\n\ndef main():\n    # 설정\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    batch_size = 32\n    num_epochs = 5  # 에폭 수 증가\n    learning_rate = 0.001\n    \n    # 데이터셋 및 데이터로더 생성\n    train_dataset = VADDataset(BASE_PATH, is_train=True)\n    test_dataset = VADDataset(BASE_PATH, is_train=False)\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True,\n        num_workers=4,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size, \n        shuffle=False,\n        num_workers=4,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    # 모델 초기화\n    input_size = 15  # MFCC(13) + RMS(1) + ZCR(1)\n    model = VADModel(input_size).to(device)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n    )\n    \n    # 학습\n    print(\"\\nStarting training...\")\n    best_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}')\n        \n        # Learning rate 조정\n        scheduler.step(train_loss)\n        \n        # 모델 저장\n        if train_loss < best_loss:\n            best_loss = train_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n    \n    # 최적 모델 로드\n    model.load_state_dict(torch.load('best_model.pth'))\n    \n    # 테스트 데이터 예측\n    print(\"\\nGenerating predictions...\")\n    predictions = predict(model, test_loader, device)\n    \n    # 제출 파일 생성\n    submission_df = pd.DataFrame({\n        'Id': np.arange(len(predictions)),\n        'Center': predictions.flatten()\n    })\n    \n    submission_path = 'submission.csv'\n    submission_df.to_csv(submission_path, index=False)\n    print(f\"\\nSubmission file saved to: {submission_path}\")\n    print(\"First few predictions:\", submission_df['Center'].head())\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:52:21.050730Z","iopub.execute_input":"2024-11-25T14:52:21.051052Z","iopub.status.idle":"2024-11-25T16:34:35.862587Z","shell.execute_reply.started":"2024-11-25T14:52:21.051025Z","shell.execute_reply":"2024-11-25T16:34:35.861532Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain WAV path: /kaggle/working/data/train/wav160\nTrain TEXT path: /kaggle/working/data/train/text\nNumber of files: 50000\nTest WAV path: /kaggle/working/data/test/wav160\nNumber of files: 10000\n\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [19:43<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 3.0112\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [19:48<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Loss: 2.7240\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [19:57<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Loss: 2.0417\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [19:22<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Loss: 1.7379\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [19:27<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Loss: 1.5522\n\nGenerating predictions...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 313/313 [03:50<00:00,  1.36it/s]","output_type":"stream"},{"name":"stdout","text":"\nSubmission file saved to: submission.csv\nFirst few predictions: 0   -0.635486\n1    0.822275\n2   -0.743630\n3   -0.789553\n4    0.351603\nName: Center, dtype: float32\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4}]}